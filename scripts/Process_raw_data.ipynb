{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "\n",
    "Aggregate precipitation increment data from the NRCS database.\n",
    "\n",
    "#### Sources: \n",
    "- NRCS interactive map: https://www.nrcs.usda.gov/wps/portal/wcc/home/\n",
    "- NRCS web report scripting (URL-to-File method): https://www.wcc.nrcs.usda.gov/web_service/NWCC_Web_Report_Scripting.txt\n",
    "\n",
    "NRCS interactive map provides precipitation increments. For SNOTEL stations, snow-adjusted precipitation increments are also available. Note that the database through the map was missing precipitation records of a few stations. The web report scripting was used to access those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import os, glob\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "Downloaded using the NRCS interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('../data_raw/NRCS_metadata.csv', skiprows=261, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow-adjusted precipitation\n",
    "Downloaded from the NRCS interactive map\n",
    "\n",
    "#### Read in Snow-adjusted precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCP_SA = pd.read_csv('../data_raw/NRCS_data_PRCP_inc_SnowAdj.csv', skiprows=264, dtype=str)\n",
    "\n",
    "## Extract precip values and ignore QA/QC flags\n",
    "PRCP_SA_val = PRCP_SA.iloc[:,1::3]\n",
    "\n",
    "## Drop columns that have all nan values\n",
    "PRCP_SA_val = PRCP_SA_val.dropna(axis=1, how='all')\n",
    "\n",
    "## Rename columns of dataframes by using information from metadata.\n",
    "\n",
    "PRCP_SA_val_full = PRCP_SA.iloc[:,1::3]\n",
    "filled_indices_SA = [PRCP_SA_val_full.columns.get_loc(PRCP_SA_val.columns[i]) for i in np.arange(PRCP_SA_val.columns.size)]\n",
    "\n",
    "PRCP_SA_val.columns = Metadata.iloc[filled_indices_SA,0]\n",
    "PRCP_SA_val.columns.name=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that all snow-adjusted records belong to SNOTEL (SNTL) stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL',\n",
       "       'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL', 'SNTL'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metadata.iloc[filled_indices_SA,2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-snow adjusted precipitation\n",
    "Downloaded from the NRCS interactive map\n",
    "\n",
    "#### Read in data only for non-SNOTEL stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_SNTL_ind = Metadata[Metadata['Network Code'] != 'SNTL'].index\n",
    "\n",
    "PRCP_inc = pd.read_csv('../data_raw/NRCS_data_PRCP_inc.csv', skiprows=264, dtype=str)\n",
    "\n",
    "## Extract precip values and ignore QA/QC flags\n",
    "PRCP_inc_val = PRCP_inc.iloc[:,1::3]\n",
    "\n",
    "## Keep only the non-SNOTEL columns\n",
    "PRCP_inc_val = PRCP_inc_val.iloc[:,non_SNTL_ind]\n",
    "\n",
    "## Rename columns\n",
    "PRCP_inc_val.columns = Metadata[Metadata['Network Code'] != 'SNTL']['Station Id'].values\n",
    "\n",
    "## Drop columns that have all nan values\n",
    "PRCP_inc_val = PRCP_inc_val.dropna(axis=1, how='all')\n",
    "\n",
    "## Drop the second column (the time series only has one value)\n",
    "PRCP_inc_val = PRCP_inc_val.drop(columns = PRCP_inc_val.columns[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the two types of precipitation data in a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_date = lambda x: (datetime.datetime.strptime(x,\"%Y-%m-%d\") + datetime.timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "PRCP_SA_val_float = PRCP_SA_val.astype('float')\n",
    "PRCP_inc_val_float = PRCP_inc_val.astype('float')\n",
    "\n",
    "NRCS_data = pd.merge(PRCP_SA['Date'], PRCP_SA_val_float, left_index=True, right_index=True)\n",
    "NRCS_data = pd.merge(NRCS_data, PRCP_inc_val_float, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increment dates\n",
    "All the precipitation values obtained from the database are daily values (24h starting at midnight local time). The dates are incremented by 1 as a personal preference, so that precipitation for each day corresponds to the previous 24h. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremented_dates = [inc_date(x) for x in NRCS_data['Date']]\n",
    "NRCS_data['Date'] = incremented_dates\n",
    "NRCS_data = NRCS_data.iloc[:-1,:]\n",
    "\n",
    "## convert datatype to float, and multiply by 25.4 to convert inches to mm.\n",
    "NRCS_data.iloc[:,1:] = NRCS_data.iloc[:,1:]*25.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add columns for Year, Month and Day to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_stripped = [datetime.datetime.strptime(item, \"%Y-%m-%d\") for item in NRCS_data['Date']]\n",
    "\n",
    "years = [item.year for item in date_stripped]\n",
    "months = [item.month for item in date_stripped]\n",
    "days = [item.day for item in date_stripped]\n",
    "\n",
    "Date_df = pd.DataFrame({'Date':NRCS_data['Date'], 'Year': years, 'Month': months, 'Day':days})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRCS_df = pd.merge(Date_df, NRCS_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional COOP network stations\n",
    "\n",
    "These were downloaded using NRCS web-report scripting. The precipitation data for these stations were missing from the data obtained using the NRCS interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_NRCSdata(prec):\n",
    "    index = prec.index[prec.values == -99.90]\n",
    "    if len(index):\n",
    "        for val in index:\n",
    "            prec.iloc[val] = np.nan\n",
    "        return\n",
    "\n",
    "\n",
    "def get_data_additional_stations(StationID):\n",
    "    datapath = '../data_raw/Additional_Stations/'\n",
    "    filenames=[]\n",
    "    for file in glob.glob( os.path.join(datapath + StationID +'*WY.csv') ):\n",
    "        filenames.append(file)\n",
    "    filenames.sort()\n",
    "    Date = []\n",
    "    daily_prec = []\n",
    "    for file in filenames:\n",
    "        data = pd.read_csv(file, skiprows=3)\n",
    "        if len(data):\n",
    "            prec = data.loc[:,'PREC.I-1 (in) '].copy()\n",
    "            clean_NRCSdata(prec)\n",
    "            Date.append(data.loc[:,'Date'].iloc[0:-1])\n",
    "            daily_prec.append((prec.iloc[1:].values - prec.iloc[0:-1].values)*25.4)\n",
    "        \n",
    "    Date = pd.concat(Date, ignore_index=True)\n",
    "    daily_prec = np.concatenate(daily_prec)\n",
    "\n",
    "    incremented_dates = [inc_date(x) for x in Date]   \n",
    "    return pd.DataFrame({'Date':incremented_dates, StationID:daily_prec})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stations = ['0484', '0761', '2192', '0951', '0909']\n",
    "\n",
    "for item in additional_stations:\n",
    "    data = get_data_additional_stations(item)\n",
    "    NRCS_df = pd.merge(NRCS_df, data, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save precipitation records as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRCS_df.to_csv('../data_processed/NRCS_dates_2008_2017.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('../data_raw/NRCS_metadata.csv', skiprows=261, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = Metadata[Metadata['Station Id'].isin(NRCS_df.columns[4:])]\n",
    "\n",
    "Metadata = Metadata[['Station Id', 'Elevation', 'Latitude', 'Longitude']]\n",
    "\n",
    "Metadata.rename(columns={'Station Id':'StationID'}, inplace=True, copy=True)\n",
    "\n",
    "Metadata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Metadata[['Elevation','Latitude','Longitude']] = Metadata[['Elevation','Latitude','Longitude']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2138</td>\n",
       "      <td>6445</td>\n",
       "      <td>37.67455</td>\n",
       "      <td>-109.36429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1030</td>\n",
       "      <td>10960</td>\n",
       "      <td>40.35098</td>\n",
       "      <td>-106.38142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0484</td>\n",
       "      <td>6240</td>\n",
       "      <td>41.03333</td>\n",
       "      <td>-107.65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>317</td>\n",
       "      <td>7440</td>\n",
       "      <td>41.05413</td>\n",
       "      <td>-107.26609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1061</td>\n",
       "      <td>9080</td>\n",
       "      <td>40.06153</td>\n",
       "      <td>-107.00955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>859</td>\n",
       "      <td>8950</td>\n",
       "      <td>41.00289</td>\n",
       "      <td>-106.90848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>864</td>\n",
       "      <td>8641</td>\n",
       "      <td>39.96450</td>\n",
       "      <td>-110.98845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>869</td>\n",
       "      <td>9540</td>\n",
       "      <td>40.34703</td>\n",
       "      <td>-106.09433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>874</td>\n",
       "      <td>11000</td>\n",
       "      <td>37.47922</td>\n",
       "      <td>-106.80170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1228</td>\n",
       "      <td>9327</td>\n",
       "      <td>39.13233</td>\n",
       "      <td>-111.35685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    StationID  Elevation  Latitude  Longitude\n",
       "0        2138       6445  37.67455 -109.36429\n",
       "1        1030      10960  40.35098 -106.38142\n",
       "2        0484       6240  41.03333 -107.65000\n",
       "3         317       7440  41.05413 -107.26609\n",
       "4        1061       9080  40.06153 -107.00955\n",
       "..        ...        ...       ...        ...\n",
       "147       859       8950  41.00289 -106.90848\n",
       "148       864       8641  39.96450 -110.98845\n",
       "149       869       9540  40.34703 -106.09433\n",
       "150       874      11000  37.47922 -106.80170\n",
       "151      1228       9327  39.13233 -111.35685\n",
       "\n",
       "[152 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save processed Metadata to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata.to_csv('../data_processed/Metadata_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
